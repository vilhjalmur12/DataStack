services:
  spark-master:
    image: bitnami/spark:3.5
    container_name: spark-master
    hostname: spark-master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      - SPARK_HISTORY_OPTS=-Dspark.history.fs.logDirectory=file:///opt/spark/logs
    volumes:
      - spark-data:/opt/spark/logs
      - spark-logs:/var/log/spark
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    ports:
      - "7077:7077"
      - "8080:8080"
      - "6066:6066"  # Add Spark Job Submission REST API

  spark-worker:
    image: bitnami/spark:3.5
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=${SPARK_WORKER_MEMORY}
      - SPARK_WORKER_CORES=${SPARK_WORKER_CORES}
    volumes:
      - spark-data:/opt/spark/logs
      - spark-logs:/var/log/spark
      - ./config/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf
    deploy:
      replicas: 2
      restart_policy:
        condition: on-failure
    ports:
      - "8081-8083:8081"


volumes:
  spark-data:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${SPARK_MOUNT_LOCATION}
  spark-logs:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${SPARK_LOGS_MOUNT_LOCATION}

