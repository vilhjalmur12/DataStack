# **Apache Spark - Big Data Processing Service**

## **ğŸ“Œ Overview**
Apache Spark is a fast and distributed data processing framework used for **big data processing, querying, and analytics**. It integrates with **Delta Lake, Presto, Trino**, and other query engines.

This document provides instructions on how to deploy and manage Spark using:
- **Docker Compose** (for local development)
- **Helm Charts** (for Kubernetes production deployments)
- **Terraform** (for Hetzner Cloud infrastructure provisioning)

---

## **ğŸ“‚ Directory Structure**
```
big-data-stack/
â”‚â”€â”€ services/
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ docker-compose.yml   # Spark standalone setup
â”‚   â”‚   â”œâ”€â”€ .env                 # Spark environment variables
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â”œâ”€â”€ spark-defaults.conf  # Spark configuration
â”‚   â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”‚   â”œâ”€â”€ entrypoint.sh    # Startup script (if needed)
â”‚
â”‚â”€â”€ helm-charts/
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ Chart.yaml
â”‚   â”‚   â”œâ”€â”€ values.yaml
â”‚   â”‚   â”œâ”€â”€ templates/
â”‚   â”‚   â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ service.yaml
â”‚   â”‚   â”‚   â”œâ”€â”€ pvc.yaml
â”‚
â”‚â”€â”€ terraform/
â”‚   â”œâ”€â”€ hetzner/
â”‚   â”‚   â”œâ”€â”€ spark.tf   # Spark infrastructure setup
â”‚
â”‚â”€â”€ helmfile.yaml  # Multi-service Helmfile deployment
â”‚â”€â”€ environments/
â”‚   â”œâ”€â”€ dev.yaml   # Helmfile values for development
â”‚   â”œâ”€â”€ prod.yaml  # Helmfile values for production
```

---

## **ğŸŒ Environment Variables**
Environment variables are stored in `.env` for Docker and `values.yaml` for Helm. Below is a brief explanation of each variable:

- `SPARK_MASTER_HOST`: The hostname of the Spark master node.
- `SPARK_WORKER_MEMORY`: Amount of memory allocated for Spark worker nodes.
- `SPARK_WORKER_CORES`: Number of CPU cores assigned to Spark worker nodes.
- `SPARK_DRIVER_MEMORY`: Memory allocated for the Spark driver process.
- `SPARK_EXECUTOR_MEMORY`: Memory allocated for each Spark executor.
- `SPARK_EXECUTOR_CORES`: Number of CPU cores assigned to each executor.

This section provides essential configurations to optimize Spark performance.
Environment variables are stored in `.env` for Docker and `values.yaml` for Helm.

### **ğŸ”¹ `.env` (Docker Compose)**
```ini
SPARK_MASTER_HOST=spark-master
SPARK_WORKER_MEMORY=4G
SPARK_WORKER_CORES=2
SPARK_DRIVER_MEMORY=2G
SPARK_EXECUTOR_MEMORY=4G
SPARK_EXECUTOR_CORES=2
```

### **ğŸ”¹ `values.yaml` (Helm)**
```yaml
spark:
  master:
    serviceType: ClusterIP
    port: 7077
    webUIPort: 8080
  worker:
    replicas: 2
    memory: "4G"
    cores: 2
  eventLog:
    enabled: true
    directory: "file:///opt/spark/logs"
nodeSelector:
  pool: np-query
```

---

## **ğŸ’» System Requirements**

### **ğŸ”¹ Development Setup (Docker Compose)**
| Resource  | Minimum |
|-----------|---------|
| CPU       | 4 vCPUs |
| RAM       | 8GB     |
| Disk      | 10GB SSD |

### **ğŸ”¹ Production Setup (Kubernetes & Hetzner)**
| Resource  | Minimum |
|-----------|---------|
| CPU       | 8 vCPUs |
| RAM       | 16GB    |
| Disk      | 50GB SSD (logs) + 100GB SSD (data) |

---

## **ğŸš€ Deployment Instructions**

### **1ï¸âƒ£ Local Development (Docker Compose)**
```sh
docker-compose up -d
```
ğŸ“Œ **Verify running services:**
```sh
docker ps
```
ğŸ“Œ **Access Spark UI:**
```sh
http://localhost:8080
```

### **2ï¸âƒ£ Kubernetes Deployment (Helm Charts)**
**1ï¸âƒ£ Deploy for Development**
```sh
helmfile -e dev apply
```

**2ï¸âƒ£ Deploy for Production**
```sh
helmfile -e prod apply
```

**3ï¸âƒ£ Verify Deployment**
```sh
kubectl get pods -n bigdata-query
kubectl get svc -n bigdata-query
```

**4ï¸âƒ£ Expose Spark UI (if needed)**
```sh
kubectl port-forward service/spark-master 8080:8080 -n bigdata-query
```
ğŸ“Œ **View UI at:** [http://localhost:8080](http://localhost:8080)

### **3ï¸âƒ£ Infrastructure Deployment (Terraform - Hetzner Cloud)**

Before applying the Terraform plan, ensure that `variables.tf` is properly configured to match your infrastructure needs. You can customize the following variables:

- `hcloud_token`: Hetzner Cloud API token.
- `region`: The region where your Hetzner instances will be deployed.
- `spark_node_count`: Number of Spark worker nodes.
- `spark_instance_type`: Instance type for Spark nodes.
- `ssh_key_name`: SSH key for secure access.

These variables can be modified in `terraform/hetzner/variables.tf` before running `terraform apply` to ensure correct resource allocation.
**1ï¸âƒ£ Initialize Terraform**
```sh
terraform init
```

**2ï¸âƒ£ Preview Changes**
```sh
terraform plan
```

**3ï¸âƒ£ Deploy Infrastructure**
```sh
terraform apply -auto-approve
```

**4ï¸âƒ£ Verify Hetzner Deployment**
```sh
hcloud server list
hcloud volume list
```

**5ï¸âƒ£ SSH into Spark Master to Verify Setup**
```sh
ssh root@$(hcloud server ip spark-master)
```
Then run:
```sh
env | grep SPARK
```
âœ… You should see:
```
SPARK_MASTER_HOST=spark-master
SPARK_MASTER_URL=spark://spark-master:7077
```

---

## **ğŸ” Troubleshooting**

### **Common Issues & Solutions**

#### **1ï¸âƒ£ Spark Master or Worker Failing to Start**
**Error:** `Failed to connect to master at spark://spark-master:7077`
- Ensure that the **Spark master service is running**.
- Verify that the **network settings allow communication** between Spark master and workers.
- Check logs using:
  ```sh
  docker logs spark-master
  kubectl logs -n bigdata-query <spark-master-pod>
  ```

#### **2ï¸âƒ£ No Applications Showing in Spark UI**
**Possible Causes & Fixes:**
- Check if **event logging is enabled**:
  ```sh
  kubectl exec -it <spark-master-pod> -n bigdata-query -- cat /opt/spark/conf/spark-defaults.conf | grep eventLog
  ```
- Ensure the **log directory is writable** and accessible to the Spark user.

#### **3ï¸âƒ£ High Memory Usage or Worker Crashes**
- Verify that **worker memory and core limits** match available resources.
- Check logs for `OutOfMemoryError` or resource exhaustion.
- Adjust Spark memory settings in `values.yaml` or `.env`:
  ```yaml
  SPARK_WORKER_MEMORY=8G
  SPARK_EXECUTOR_MEMORY=6G
  ```

#### **4ï¸âƒ£ Connection Issues Between Spark and External Query Engines**
- Ensure that **Presto, Trino, or Delta Lake are reachable** from Spark.
- Validate network settings and make sure all required services are in the same **Kubernetes namespace**.

### **General Debugging Steps**

### **1ï¸âƒ£ Check Logs**
```sh
docker logs spark-master
kubectl logs -n bigdata-query <spark-master-pod>
```

### **2ï¸âƒ£ Check Cluster Status**
```sh
kubectl get pods -n bigdata-query -o wide
```

### **3ï¸âƒ£ Restart Services**
```sh
docker-compose restart spark-master
kubectl rollout restart deployment spark-master -n bigdata-query
```

### **1ï¸âƒ£ Check Logs**
```sh
docker logs spark-master
kubectl logs -n bigdata-query <spark-master-pod>
```

### **2ï¸âƒ£ Check Cluster Status**
```sh
kubectl get pods -n bigdata-query -o wide
```

### **3ï¸âƒ£ Restart Services**
```sh
docker-compose restart spark-master
kubectl rollout restart deployment spark-master -n bigdata-query
```

---

## **ğŸ“Œ Summary**
| **Deployment Type** | **Command** |
|------------------|------------|
| **Docker Compose (local)** | `docker-compose up -d` |
| **Helm (Kubernetes - Dev)** | `helmfile -e dev apply` |
| **Helm (Kubernetes - Prod)** | `helmfile -e prod apply` |
| **Terraform (Hetzner)** | `terraform apply -auto-approve` |

âœ… **Spark is now fully documented and ready for deployment!** ğŸš€


